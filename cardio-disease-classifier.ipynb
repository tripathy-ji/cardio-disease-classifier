{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5981a4a434e475fa7c46acd6c1ceb5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e006224d3cca4a2f98ddb2dc136c2bd3",
              "IPY_MODEL_9461dd0769dc4843842a4ec524716443",
              "IPY_MODEL_fbb74c83203c4113a45ce7cedcbfaf7b"
            ],
            "layout": "IPY_MODEL_ef2ca5816c6e4f53bb36194b1b2d2e63"
          }
        },
        "e006224d3cca4a2f98ddb2dc136c2bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51884a23cc1c47c1ac361592b25e1262",
            "placeholder": "​",
            "style": "IPY_MODEL_b56d64d89c314b6cb8ffa693de7940b0",
            "value": "Training: 100%"
          }
        },
        "9461dd0769dc4843842a4ec524716443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b08a128be6074f959fffee2c129a7604",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2db3deb98525466e8614aaae130c48b8",
            "value": 1
          }
        },
        "fbb74c83203c4113a45ce7cedcbfaf7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2eb5b6fb55204f4dab4fc57620df7cbb",
            "placeholder": "​",
            "style": "IPY_MODEL_93075c50875c4304afaf23386b633884",
            "value": " 1/1 [01:27&lt;00:00, 87.58s/it]"
          }
        },
        "ef2ca5816c6e4f53bb36194b1b2d2e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51884a23cc1c47c1ac361592b25e1262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b56d64d89c314b6cb8ffa693de7940b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b08a128be6074f959fffee2c129a7604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2db3deb98525466e8614aaae130c48b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2eb5b6fb55204f4dab4fc57620df7cbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93075c50875c4304afaf23386b633884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc9e04520fe748d69722433386b02a72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_162ad7ee0ead434d8fdaecf9fc2313bb",
              "IPY_MODEL_c0a74134c6794b33b19452dc19b907f2",
              "IPY_MODEL_72d0101c749a487ab04791377d337e75"
            ],
            "layout": "IPY_MODEL_6d90f2b1ff4f40389978b674a601583d"
          }
        },
        "162ad7ee0ead434d8fdaecf9fc2313bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4c0e3bf7ff544da9cf52c2a0fe2aecc",
            "placeholder": "​",
            "style": "IPY_MODEL_8256006adabd4ea08054202d7dfca7ae",
            "value": "Training: 100%"
          }
        },
        "c0a74134c6794b33b19452dc19b907f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f562eabde33545e7b072800862701679",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97d513570cfe409c9c6354926e56cce0",
            "value": 1
          }
        },
        "72d0101c749a487ab04791377d337e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b3c9946de5c4d5f869e50214d13c9e9",
            "placeholder": "​",
            "style": "IPY_MODEL_c60113fd759943e5b4663e2f49ea107b",
            "value": " 1/1 [07:03&lt;00:00, 423.06s/it]"
          }
        },
        "6d90f2b1ff4f40389978b674a601583d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4c0e3bf7ff544da9cf52c2a0fe2aecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8256006adabd4ea08054202d7dfca7ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f562eabde33545e7b072800862701679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97d513570cfe409c9c6354926e56cce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b3c9946de5c4d5f869e50214d13c9e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60113fd759943e5b4663e2f49ea107b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pandas scikit-learn xgboost joblib tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwfx-42gOIj-",
        "outputId": "2bca8505-bdb6-4320-db0b-a030084cef30"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import joblib\n",
        "import zipfile\n",
        "\n",
        "# Upload kaggle.json file before this step\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# ✅ Set up custom Kaggle config path\n",
        "print(\"🔑 Setting up Kaggle API...\")\n",
        "os.makedirs(\"/content/.kaggle\", exist_ok=True)\n",
        "with open(\"/content/.kaggle/kaggle.json\", \"wb\") as f:\n",
        "    f.write(uploaded[\"kaggle.json\"])\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/.kaggle\"\n",
        "\n",
        "# ✅ Download dataset\n",
        "print(\"⬇️ Downloading dataset...\")\n",
        "!kaggle datasets download -d sulianova/cardiovascular-disease-dataset\n",
        "\n",
        "# ✅ Unzip dataset\n",
        "print(\"🗂️ Unzipping dataset...\")\n",
        "with zipfile.ZipFile(\"cardiovascular-disease-dataset.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"data\")\n",
        "\n",
        "# ✅ Load dataset\n",
        "print(\"📄 Loading dataset...\")\n",
        "df = pd.read_csv(\"data/cardio_train.csv\", sep=\";\")\n",
        "\n",
        "# ✅ Preprocessing\n",
        "print(\"🧼 Preprocessing...\")\n",
        "df[\"age\"] = (df[\"age\"] / 365).astype(int)\n",
        "df = df[(df[\"ap_hi\"] > 0) & (df[\"ap_hi\"] < 300)]\n",
        "df = df[(df[\"ap_lo\"] > 0) & (df[\"ap_lo\"] < 200)]\n",
        "df = df[df[\"ap_hi\"] >= df[\"ap_lo\"]]\n",
        "df.drop(\"id\", axis=1, inplace=True)\n",
        "\n",
        "cat_cols = [\"gender\", \"cholesterol\", \"gluc\", \"smoke\", \"alco\", \"active\"]\n",
        "num_cols = [\"age\", \"height\", \"weight\", \"ap_hi\", \"ap_lo\"]\n",
        "\n",
        "for col in tqdm(cat_cols, desc=\"🔤 Encoding categorical\"):\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "X = df.drop(\"cardio\", axis=1)\n",
        "y = df[\"cardio\"]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ✅ Train Random Forest\n",
        "print(\"🌲 Training RandomForest...\")\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "print(f\"✅ RF Acc: {accuracy_score(y_val, rf.predict(X_val)):.4f} | ROC-AUC: {roc_auc_score(y_val, rf.predict_proba(X_val)[:,1]):.4f}\")\n",
        "\n",
        "# ✅ Train XGBoost\n",
        "print(\"⚡ Training XGBoost...\")\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "print(f\"✅ XGB Acc: {accuracy_score(y_val, xgb.predict(X_val)):.4f} | ROC-AUC: {roc_auc_score(y_val, xgb.predict_proba(X_val)[:,1]):.4f}\")\n",
        "\n",
        "# ✅ Train Neural Network\n",
        "print(\"🧠 Training Neural Network...\")\n",
        "nn = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300, early_stopping=True, random_state=42)\n",
        "nn.fit(X_train, y_train)\n",
        "print(f\"✅ NN Acc: {accuracy_score(y_val, nn.predict(X_val)):.4f} | ROC-AUC: {roc_auc_score(y_val, nn.predict_proba(X_val)[:,1]):.4f}\")\n",
        "\n",
        "# Save best model (XGBoost)\n",
        "joblib.dump(xgb, \"best_model.pkl\")\n",
        "\n",
        "# ✅ Prediction function\n",
        "def predict_from_input(input_dict):\n",
        "    try:\n",
        "        input_df = pd.DataFrame([input_dict])\n",
        "        input_df[num_cols] = scaler.transform(input_df[num_cols])\n",
        "        for col in cat_cols:\n",
        "            input_df[col] = LabelEncoder().fit_transform([input_dict[col]])[0]\n",
        "        input_df = input_df[X.columns.tolist()]\n",
        "        pred = xgb.predict(input_df)[0]\n",
        "        return f\"🔍 Prediction: {'HIGH risk' if pred else 'LOW risk'}\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Prediction error: {e}\"\n",
        "\n",
        "# ✅ Sample prediction\n",
        "sample = {\n",
        "    \"age\": 50, \"height\": 170, \"weight\": 70, \"ap_hi\": 120, \"ap_lo\": 80,\n",
        "    \"gender\": 1, \"cholesterol\": 1, \"gluc\": 1, \"smoke\": 0, \"alco\": 0, \"active\": 1\n",
        "}\n",
        "print(\"📦 Sample prediction:\")\n",
        "print(predict_from_input(sample))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "4TYF2oXdO85z",
        "outputId": "38430e7d-5a12-406b-a887-5ec764d1be34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-eae42582-1a0d-4a1a-a53c-643e3ff5574e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-eae42582-1a0d-4a1a-a53c-643e3ff5574e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "🔑 Setting up Kaggle API...\n",
            "⬇️ Downloading dataset...\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset\n",
            "License(s): unknown\n",
            "Downloading cardiovascular-disease-dataset.zip to /content\n",
            "  0% 0.00/742k [00:00<?, ?B/s]\n",
            "100% 742k/742k [00:00<00:00, 264MB/s]\n",
            "🗂️ Unzipping dataset...\n",
            "📄 Loading dataset...\n",
            "🧼 Preprocessing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔤 Encoding categorical: 100%|██████████| 6/6 [00:00<00:00, 258.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌲 Training RandomForest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ RF Acc: 0.7279 | ROC-AUC: 0.7981\n",
            "⚡ Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [13:08:35] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ XGB Acc: 0.7296 | ROC-AUC: 0.7955\n",
            "🧠 Training Neural Network...\n",
            "✅ NN Acc: 0.7290 | ROC-AUC: 0.7955\n",
            "📦 Sample prediction:\n",
            "🔍 Prediction: LOW risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 0: Install dependencies\n",
        "!pip install -q kaggle xgboost tqdm scikit-learn\n",
        "\n",
        "# STEP 1: Upload your kaggle.json manually or ensure it exists in the runtime\n",
        "import os, zipfile\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "!mv kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# STEP 2: Download dataset from Kaggle\n",
        "print(\"🔑 Setting up Kaggle API...\")\n",
        "!kaggle datasets download -d sulianova/cardiovascular-disease-dataset\n",
        "\n",
        "# STEP 3: Unzip dataset\n",
        "print(\"🗂️ Unzipping dataset...\")\n",
        "with zipfile.ZipFile(\"cardiovascular-disease-dataset.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")\n",
        "\n",
        "# STEP 4: Import and preprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"📄 Loading dataset...\")\n",
        "df = pd.read_csv(\"/content/cardio_train.csv\", sep=';')\n",
        "df.drop(columns=['id'], inplace=True)\n",
        "\n",
        "print(\"🧼 Preprocessing...\")\n",
        "# Upsample minority class\n",
        "df_major = df[df.cardio == 0]\n",
        "df_minor = df[df.cardio == 1]\n",
        "df_minor_upsampled = resample(df_minor, replace=True, n_samples=len(df_major), random_state=42)\n",
        "df = pd.concat([df_major, df_minor_upsampled]).sample(frac=1, random_state=42)\n",
        "\n",
        "# Features and labels\n",
        "X = df.drop(\"cardio\", axis=1)\n",
        "y = df[\"cardio\"]\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Models\n",
        "print(\"🌲 Training RandomForest...\")\n",
        "rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "print(\"⚡ Training XGBoost...\")\n",
        "xgb = XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.05, use_label_encoder=False, eval_metric=\"logloss\")\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_pred = xgb.predict(X_test)\n",
        "\n",
        "print(\"🧠 Training Neural Network...\")\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', max_iter=300, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "mlp_pred = mlp.predict(X_test)\n",
        "\n",
        "# Voting Ensemble\n",
        "print(\"🧠 Combining all (Voting Classifier)...\")\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('rf', rf), ('xgb', xgb), ('mlp', mlp)\n",
        "], voting='soft')\n",
        "\n",
        "ensemble.fit(X_train, y_train)\n",
        "ensemble_pred = ensemble.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "acc = accuracy_score(y_test, ensemble_pred)\n",
        "roc = roc_auc_score(y_test, ensemble_pred)\n",
        "cm = confusion_matrix(y_test, ensemble_pred)\n",
        "\n",
        "print(f\"\\n✅ Ensemble Accuracy: {acc:.4f}\")\n",
        "print(f\"✅ ROC-AUC Score: {roc:.4f}\")\n",
        "print(f\"🧾 Confusion Matrix:\\n{cm}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "aAMK0iqpPdTE",
        "outputId": "95666e73-eaa8-4457-ad11-0af42cc4ff63"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7405e4b2-8663-4699-9dc7-e65ad6534a23\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7405e4b2-8663-4699-9dc7-e65ad6534a23\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n",
            "🔑 Setting up Kaggle API...\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset\n",
            "License(s): unknown\n",
            "cardiovascular-disease-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "🗂️ Unzipping dataset...\n",
            "📄 Loading dataset...\n",
            "🧼 Preprocessing...\n",
            "🌲 Training RandomForest...\n",
            "⚡ Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [13:11:01] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Training Neural Network...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Combining all (Voting Classifier)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [13:15:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Ensemble Accuracy: 0.7468\n",
            "✅ ROC-AUC Score: 0.7468\n",
            "🧾 Confusion Matrix:\n",
            "[[5497 1509]\n",
            " [2038 4965]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- SETUP --------------------\n",
        "print(\"🔧 Installing & importing dependencies...\")\n",
        "!pip install -q kaggle tqdm xgboost scikit-learn pandas matplotlib seaborn\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "\n",
        "# -------------------- AUTH & DOWNLOAD --------------------\n",
        "print(\"🔑 Setting up Kaggle API...\")\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "!mv kaggle.json /root/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "print(\"⬇️ Downloading dataset...\")\n",
        "!kaggle datasets download -d sulianova/cardiovascular-disease-dataset\n",
        "with zipfile.ZipFile(\"cardiovascular-disease-dataset.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"data\")\n",
        "\n",
        "# -------------------- LOAD & PREPROCESS --------------------\n",
        "print(\"📄 Loading dataset...\")\n",
        "df = pd.read_csv(\"data/cardio_train.csv\", sep=\";\")\n",
        "\n",
        "print(\"🧼 Preprocessing...\")\n",
        "df.drop(\"id\", axis=1, inplace=True)\n",
        "df[\"age\"] = df[\"age\"] // 365  # Convert age from days to years\n",
        "\n",
        "# Remove outliers (height and weight beyond quantile thresholds)\n",
        "df = df[(df[\"height\"] > df[\"height\"].quantile(0.01)) & (df[\"height\"] < df[\"height\"].quantile(0.99))]\n",
        "df = df[(df[\"weight\"] > df[\"weight\"].quantile(0.01)) & (df[\"weight\"] < df[\"weight\"].quantile(0.99))]\n",
        "\n",
        "X = df.drop(\"cardio\", axis=1)\n",
        "y = df[\"cardio\"]\n",
        "\n",
        "print(\"📊 Splitting dataset...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -------------------- TRAIN MODELS --------------------\n",
        "print(\"🌲 Training RandomForest...\")\n",
        "rf = RandomForestClassifier(n_estimators=150, max_depth=12, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"⚡ Training XGBoost...\")\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=150, learning_rate=0.08, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"🧠 Training NeuralNet...\")\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# -------------------- ENSEMBLE --------------------\n",
        "print(\"🧠 Combining all (Voting Classifier)...\")\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('rf', rf),\n",
        "    ('xgb', xgb_model),\n",
        "    ('nn', mlp)\n",
        "], voting='soft')\n",
        "\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "# -------------------- EVALUATE --------------------\n",
        "print(\"📈 Evaluating ensemble model...\")\n",
        "y_pred = ensemble.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, ensemble.predict_proba(X_test)[:, 1])\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n✅ Ensemble Accuracy: {acc:.4f}\")\n",
        "print(f\"✅ ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(\"🧾 Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# -------------------- TEST SAMPLE --------------------\n",
        "def predict_user_input(model, input_dict):\n",
        "    input_df = pd.DataFrame([input_dict])\n",
        "    return model.predict(input_df)[0]\n",
        "\n",
        "sample = {\n",
        "    \"age\": 50, \"gender\": 1, \"height\": 165, \"weight\": 72,\n",
        "    \"ap_hi\": 120, \"ap_lo\": 80, \"cholesterol\": 1,\n",
        "    \"gluc\": 1, \"smoke\": 0, \"alco\": 0, \"active\": 1\n",
        "}\n",
        "\n",
        "print(\"\\n📦 Sample prediction:\")\n",
        "result = predict_user_input(ensemble, sample)\n",
        "print(\"🔍 Prediction:\", \"HIGH risk\" if result == 1 else \"LOW risk\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIB0FZzCV1Fu",
        "outputId": "c8efaef9-079c-4a7d-fbf1-37004f89fbcb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Installing & importing dependencies...\n",
            "🔑 Setting up Kaggle API...\n",
            "mv: cannot stat 'kaggle.json': No such file or directory\n",
            "⬇️ Downloading dataset...\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/.kaggle/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset\n",
            "License(s): unknown\n",
            "cardiovascular-disease-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "📄 Loading dataset...\n",
            "🧼 Preprocessing...\n",
            "📊 Splitting dataset...\n",
            "🌲 Training RandomForest...\n",
            "⚡ Training XGBoost...\n",
            "🧠 Training NeuralNet...\n",
            "🧠 Combining all (Voting Classifier)...\n",
            "📈 Evaluating ensemble model...\n",
            "\n",
            "✅ Ensemble Accuracy: 0.7368\n",
            "✅ ROC-AUC Score: 0.8036\n",
            "🧾 Confusion Matrix:\n",
            "[[5234 1500]\n",
            " [2025 4633]]\n",
            "\n",
            "📦 Sample prediction:\n",
            "🔍 Prediction: LOW risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- SETUP --------------------\n",
        "print(\"🔧 Installing dependencies...\")\n",
        "!pip install -q kagglehub tqdm xgboost scikit-learn pandas matplotlib seaborn\n",
        "\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# -------------------- DOWNLOAD DATA --------------------\n",
        "print(\"⬇️ Downloading dataset from KaggleHub...\")\n",
        "dataset_path = kagglehub.dataset_download(\"sulianova/cardiovascular-disease-dataset\")\n",
        "print(\"📁 Path to dataset files:\", dataset_path)\n",
        "\n",
        "# -------------------- LOAD & CLEAN --------------------\n",
        "print(\"📄 Loading dataset...\")\n",
        "df = pd.read_csv(f\"{dataset_path}/cardio_train.csv\", sep=\";\")\n",
        "df.drop(\"id\", axis=1, inplace=True)\n",
        "df[\"age\"] = df[\"age\"] // 365  # convert days to years\n",
        "\n",
        "print(\"🧼 Cleaning and preprocessing...\")\n",
        "# Remove outliers\n",
        "df = df[(df[\"height\"] > df[\"height\"].quantile(0.01)) & (df[\"height\"] < df[\"height\"].quantile(0.99))]\n",
        "df = df[(df[\"weight\"] > df[\"weight\"].quantile(0.01)) & (df[\"weight\"] < df[\"weight\"].quantile(0.99))]\n",
        "\n",
        "# Split data\n",
        "X = df.drop(\"cardio\", axis=1)\n",
        "y = df[\"cardio\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -------------------- TRAIN MODELS --------------------\n",
        "print(\"🌲 Training RandomForest...\")\n",
        "rf = RandomForestClassifier(n_estimators=150, max_depth=12, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"⚡ Training XGBoost...\")\n",
        "xgb_model = xgb.XGBClassifier(n_estimators=150, learning_rate=0.08, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"🧠 Training Neural Network...\")\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# -------------------- ENSEMBLE --------------------\n",
        "print(\"🧠 Combining all models with VotingClassifier...\")\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('rf', rf),\n",
        "    ('xgb', xgb_model),\n",
        "    ('mlp', mlp)\n",
        "], voting='soft')\n",
        "\n",
        "ensemble.fit(X_train, y_train)\n",
        "\n",
        "# -------------------- EVALUATE --------------------\n",
        "print(\"📊 Evaluating ensemble...\")\n",
        "y_pred = ensemble.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, ensemble.predict_proba(X_test)[:, 1])\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n✅ Ensemble Accuracy: {acc:.4f}\")\n",
        "print(f\"✅ ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(\"🧾 Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# -------------------- PREDICT SAMPLE --------------------\n",
        "def predict_sample(model, features: dict):\n",
        "    df_input = pd.DataFrame([features])\n",
        "    prediction = model.predict(df_input)[0]\n",
        "    return \"HIGH risk\" if prediction == 1 else \"LOW risk\"\n",
        "\n",
        "sample = {\n",
        "    \"age\": 50, \"gender\": 1, \"height\": 165, \"weight\": 72,\n",
        "    \"ap_hi\": 120, \"ap_lo\": 80, \"cholesterol\": 1,\n",
        "    \"gluc\": 1, \"smoke\": 0, \"alco\": 0, \"active\": 1\n",
        "}\n",
        "\n",
        "print(\"\\n🔎 Sample Prediction:\")\n",
        "print(\"🔍 Risk level:\", predict_sample(ensemble, sample))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvYQ3ADDbfZj",
        "outputId": "f5142e1c-8617-48cb-f92d-3a1f9acca872"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Installing dependencies...\n",
            "⬇️ Downloading dataset from KaggleHub...\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/sulianova/cardiovascular-disease-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 742k/742k [00:00<00:00, 78.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "📁 Path to dataset files: /root/.cache/kagglehub/datasets/sulianova/cardiovascular-disease-dataset/versions/1\n",
            "📄 Loading dataset...\n",
            "🧼 Cleaning and preprocessing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌲 Training RandomForest...\n",
            "⚡ Training XGBoost...\n",
            "🧠 Training Neural Network...\n",
            "🧠 Combining all models with VotingClassifier...\n",
            "📊 Evaluating ensemble...\n",
            "\n",
            "✅ Ensemble Accuracy: 0.7368\n",
            "✅ ROC-AUC Score: 0.8036\n",
            "🧾 Confusion Matrix:\n",
            "[[5234 1500]\n",
            " [2025 4633]]\n",
            "\n",
            "🔎 Sample Prediction:\n",
            "🔍 Risk level: LOW risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔧 Install dependencies\n",
        "!pip install -q kagglehub tqdm imbalanced-learn xgboost\n",
        "\n",
        "# 📦 Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# 🔄 Load dataset\n",
        "print(\"⬇️ Downloading dataset from KaggleHub...\")\n",
        "path = kagglehub.dataset_download(\"sulianova/cardiovascular-disease-dataset\")\n",
        "print(\"📁 Path to dataset files:\", path)\n",
        "\n",
        "# 📄 Load CSV\n",
        "df = pd.read_csv(\"/kaggle/input/cardiovascular-disease-dataset/cardio_train.csv\", sep=\";\")\n",
        "\n",
        "# 🧼 Preprocess\n",
        "print(\"🧼 Cleaning and preprocessing...\")\n",
        "if 'id' in df.columns:\n",
        "    df.drop('id', axis=1, inplace=True)\n",
        "\n",
        "X = df.drop('cardio', axis=1)\n",
        "y = df['cardio']\n",
        "\n",
        "# ⚖️ Balance data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# 🎯 Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)\n",
        "\n",
        "# 🧪 Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 🧠 Models\n",
        "print(\"🌲 Training RandomForest...\")\n",
        "rf = RandomForestClassifier(n_estimators=300, max_depth=15, random_state=42)\n",
        "\n",
        "print(\"⚡ Training XGBoost...\")\n",
        "xgb = XGBClassifier(n_estimators=300, max_depth=7, learning_rate=0.05, subsample=0.8,\n",
        "                    use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "print(\"🧠 Training Neural Network...\")\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=400, alpha=0.0005, solver='adam', random_state=42)\n",
        "\n",
        "# 🤝 Voting Classifier\n",
        "print(\"🧠 Combining all models with VotingClassifier...\")\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('rf', rf),\n",
        "    ('xgb', xgb),\n",
        "    ('mlp', mlp)\n",
        "], voting='soft')\n",
        "\n",
        "# 📈 Fit model with tqdm\n",
        "print(\"🏋️ Training Ensemble...\")\n",
        "for _ in tqdm(range(1), desc=\"Training\"):\n",
        "    ensemble.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 🎯 Evaluate\n",
        "y_pred = ensemble.predict(X_test_scaled)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, ensemble.predict_proba(X_test_scaled)[:, 1])\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"\\n✅ Ensemble Accuracy:\", round(acc, 4))\n",
        "print(\"✅ ROC-AUC Score:\", round(roc_auc, 4))\n",
        "print(\"🧾 Confusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# 🔍 Prediction sample\n",
        "sample = X_test.iloc[0:1]\n",
        "risk = ensemble.predict(scaler.transform(sample))[0]\n",
        "print(\"\\n🔎 Sample Prediction:\")\n",
        "print(\"🔍 Risk level:\", \"HIGH risk\" if risk else \"LOW risk\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYIuEhLJdqWV",
        "outputId": "07be484e-9b08-4838-cd55-5803657b6bd2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading dataset from KaggleHub...\n",
            "📁 Path to dataset files: /kaggle/input/cardiovascular-disease-dataset\n",
            "🧼 Cleaning and preprocessing...\n",
            "🌲 Training RandomForest...\n",
            "⚡ Training XGBoost...\n",
            "🧠 Training Neural Network...\n",
            "🧠 Combining all models with VotingClassifier...\n",
            "🏋️ Training Ensemble...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [13:27:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "Training: 100%|██████████| 1/1 [06:11<00:00, 371.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Ensemble Accuracy: 0.7412\n",
            "✅ ROC-AUC Score: 0.8025\n",
            "🧾 Confusion Matrix:\n",
            " [[5372 1552]\n",
            " [2074 5011]]\n",
            "\n",
            "🔎 Sample Prediction:\n",
            "🔍 Risk level: HIGH risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Full Improved Cardiovascular Disease Classification Pipeline\n",
        "# ✅ Includes: Scaling, Outlier Handling, SMOTE, RF+XGB+MLP, Stacking, EarlyStopping, TQDM\n",
        "\n",
        "# ---- STEP 1: Install Required Libraries ----\n",
        "!pip install -q kaggle kagglehub xgboost lightgbm imbalanced-learn tqdm\n",
        "\n",
        "# ---- STEP 2: Import Libraries ----\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tqdm.notebook import tqdm\n",
        "import kagglehub\n",
        "\n",
        "# ---- STEP 3: Download Dataset ----\n",
        "print(\"⬇️ Downloading dataset from KaggleHub...\")\n",
        "path = kagglehub.dataset_download(\"sulianova/cardiovascular-disease-dataset\")\n",
        "print(\"📁 Path to dataset files:\", path)\n",
        "\n",
        "# ---- STEP 4: Load Data ----\n",
        "df = pd.read_csv(f\"{path}/cardio_train.csv\", sep=';')\n",
        "df.drop(columns=['id'], inplace=True)\n",
        "\n",
        "# ---- STEP 5: Clean & Engineer Features ----\n",
        "print(\"🧼 Cleaning and preprocessing...\")\n",
        "# Remove outliers\n",
        "q_low, q_hi = df.quantile(0.01), df.quantile(0.99)\n",
        "for col in ['ap_hi', 'ap_lo', 'weight', 'height']:\n",
        "    df = df[(df[col] >= q_low[col]) & (df[col] <= q_hi[col])]\n",
        "\n",
        "# Fix invalid blood pressure\n",
        "df = df[df['ap_hi'] > df['ap_lo']]\n",
        "\n",
        "# Feature engineering\n",
        "df['bmi'] = df['weight'] / ((df['height']/100)**2)\n",
        "\n",
        "# ---- STEP 6: Split Features ----\n",
        "X = df.drop(columns=['cardio'])\n",
        "y = df['cardio']\n",
        "\n",
        "# Normalize features\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# ---- STEP 7: Train-Test Split ----\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- STEP 8: Balance Data ----\n",
        "print(\"🧪 Applying SMOTE to balance classes...\")\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# ---- STEP 9: Initialize Models ----\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "xgb = XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300, early_stopping=True, random_state=42)\n",
        "\n",
        "# ---- STEP 10: Build Ensemble (Stacking) ----\n",
        "print(\"🧠 Combining all models with StackingClassifier...\")\n",
        "ensemble = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf),\n",
        "        ('xgb', xgb),\n",
        "        ('mlp', mlp)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(),\n",
        "    passthrough=True,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# ---- STEP 11: Train Model ----\n",
        "print(\"🏋️ Training Ensemble...\")\n",
        "for _ in tqdm(range(1), desc=\"Training\"):\n",
        "    ensemble.fit(X_train_res, y_train_res)\n",
        "\n",
        "# ---- STEP 12: Evaluation ----\n",
        "y_pred = ensemble.predict(X_test)\n",
        "y_prob = ensemble.predict_proba(X_test)[:, 1]\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "roc = roc_auc_score(y_test, y_prob)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n✅ Ensemble Accuracy: {acc:.4f}\")\n",
        "print(f\"✅ ROC-AUC Score: {roc:.4f}\")\n",
        "print(\"🧾 Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# ---- STEP 13: Sample Prediction ----\n",
        "sample = X_test[0].reshape(1, -1)\n",
        "sample_pred = ensemble.predict(sample)[0]\n",
        "print(\"\\n🔎 Sample Prediction:\")\n",
        "print(\"🔍 Risk level:\", \"HIGH risk\" if sample_pred else \"LOW risk\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309,
          "referenced_widgets": [
            "a5981a4a434e475fa7c46acd6c1ceb5b",
            "e006224d3cca4a2f98ddb2dc136c2bd3",
            "9461dd0769dc4843842a4ec524716443",
            "fbb74c83203c4113a45ce7cedcbfaf7b",
            "ef2ca5816c6e4f53bb36194b1b2d2e63",
            "51884a23cc1c47c1ac361592b25e1262",
            "b56d64d89c314b6cb8ffa693de7940b0",
            "b08a128be6074f959fffee2c129a7604",
            "2db3deb98525466e8614aaae130c48b8",
            "2eb5b6fb55204f4dab4fc57620df7cbb",
            "93075c50875c4304afaf23386b633884"
          ]
        },
        "id": "JN2V2WjHf7-S",
        "outputId": "bf38a1eb-9d32-48b8-8598-1d0db03d166a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading dataset from KaggleHub...\n",
            "📁 Path to dataset files: /kaggle/input/cardiovascular-disease-dataset\n",
            "🧼 Cleaning and preprocessing...\n",
            "🧪 Applying SMOTE to balance classes...\n",
            "🧠 Combining all models with StackingClassifier...\n",
            "🏋️ Training Ensemble...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5981a4a434e475fa7c46acd6c1ceb5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Ensemble Accuracy: 0.7308\n",
            "✅ ROC-AUC Score: 0.7976\n",
            "🧾 Confusion Matrix:\n",
            "[[5253 1422]\n",
            " [2123 4372]]\n",
            "\n",
            "🔎 Sample Prediction:\n",
            "🔍 Risk level: HIGH risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q imbalanced-learn\n"
      ],
      "metadata": {
        "id": "R_UCEAT7hjNf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔧 Install dependencies\n",
        "!pip install -q kagglehub imbalanced-learn xgboost tqdm\n",
        "\n",
        "# 📚 Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n",
        "\n",
        "# ⬇️ Download dataset using KaggleHub\n",
        "print(\"⬇️ Downloading dataset from KaggleHub...\")\n",
        "path = kagglehub.dataset_download(\"sulianova/cardiovascular-disease-dataset\")\n",
        "print(\"📁 Path to dataset files:\", path)\n",
        "\n",
        "# 📄 Load the dataset\n",
        "print(\"📄 Loading dataset...\")\n",
        "df = pd.read_csv(os.path.join(path, \"cardio_train.csv\"), sep=';')\n",
        "\n",
        "# 🧼 Preprocessing\n",
        "print(\"🧼 Cleaning and preprocessing...\")\n",
        "df.drop(columns=[\"id\"], inplace=True)\n",
        "df[\"age\"] = (df[\"age\"] / 365).astype(int)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(\"cardio\", axis=1)\n",
        "y = df[\"cardio\"]\n",
        "\n",
        "# 🧪 Apply SMOTE\n",
        "print(\"🧪 Applying SMOTE to balance classes...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# 🔀 Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# 🔨 Define models\n",
        "print(\"⚙️ Defining base models...\")\n",
        "rf = RandomForestClassifier(n_estimators=150, max_depth=20, random_state=42, n_jobs=-1)\n",
        "xgb = XGBClassifier(n_estimators=150, learning_rate=0.05, max_depth=5, use_label_encoder=False, eval_metric='logloss')\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=300, random_state=42)\n",
        "\n",
        "# 🧠 Combine models using Stacking\n",
        "print(\"🧠 Combining all models with StackingClassifier...\")\n",
        "estimators = [\n",
        "    (\"rf\", rf),\n",
        "    (\"xgb\", xgb),\n",
        "    (\"mlp\", mlp)\n",
        "]\n",
        "stacked_model = StackingClassifier(estimators=estimators, final_estimator=RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "\n",
        "# 🏋️ Training\n",
        "print(\"🏋️ Training Ensemble...\")\n",
        "for _ in tqdm(range(1), desc=\"Training\"):\n",
        "    stacked_model.fit(X_train, y_train)\n",
        "\n",
        "# 📈 Evaluation\n",
        "y_pred = stacked_model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "roc = roc_auc_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n✅ Ensemble Accuracy: {acc:.4f}\")\n",
        "print(f\"✅ ROC-AUC Score: {roc:.4f}\")\n",
        "print(\"🧾 Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# 🔍 Prediction sample\n",
        "sample_input = {\n",
        "    \"age\": 50, \"gender\": 1, \"height\": 165, \"weight\": 70, \"ap_hi\": 120,\n",
        "    \"ap_lo\": 80, \"cholesterol\": 1, \"gluc\": 1, \"smoke\": 0, \"alco\": 0, \"active\": 1\n",
        "}\n",
        "sample_df = pd.DataFrame([sample_input])\n",
        "sample_pred = stacked_model.predict(sample_df)[0]\n",
        "print(\"\\n🔎 Sample Prediction:\")\n",
        "print(\"🔍 Risk level:\", \"HIGH risk\" if sample_pred == 1 else \"LOW risk\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkRVjt4Mhs_x",
        "outputId": "d94d4533-29a4-4d22-c90a-45807712c976"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading dataset from KaggleHub...\n",
            "📁 Path to dataset files: /kaggle/input/cardiovascular-disease-dataset\n",
            "📄 Loading dataset...\n",
            "🧼 Cleaning and preprocessing...\n",
            "🧪 Applying SMOTE to balance classes...\n",
            "⚙️ Defining base models...\n",
            "🧠 Combining all models with StackingClassifier...\n",
            "🏋️ Training Ensemble...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [13:35:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [13:37:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [13:37:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [13:37:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [13:37:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [13:37:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "Training: 100%|██████████| 1/1 [03:35<00:00, 215.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Ensemble Accuracy: 0.7176\n",
            "✅ ROC-AUC Score: 0.7178\n",
            "🧾 Confusion Matrix:\n",
            "[[5110 1814]\n",
            " [2142 4943]]\n",
            "\n",
            "🔎 Sample Prediction:\n",
            "🔍 Risk level: LOW risk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub lightgbm xgboost imbalanced-learn tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI_TAWqnpF-V",
        "outputId": "76cf07af-2e6f-4e0e-c835-8c2af48b594f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.6.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.16.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔧 Install dependencies\n",
        "!pip install -q kagglehub lightgbm xgboost imbalanced-learn tqdm\n",
        "\n",
        "# 📚 Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ⬇️ Download dataset from KaggleHub\n",
        "print(\"⬇️ Downloading dataset from KaggleHub...\")\n",
        "path = kagglehub.dataset_download(\"sulianova/cardiovascular-disease-dataset\")\n",
        "print(\"📁 Path to dataset files:\", path)\n",
        "\n",
        "# 📄 Load dataset\n",
        "print(\"📄 Loading dataset...\")\n",
        "df = pd.read_csv(f\"{path}/cardio_train.csv\", sep=';')\n",
        "df.drop('id', axis=1, inplace=True)\n",
        "\n",
        "# 🧼 Preprocessing\n",
        "print(\"🧼 Cleaning and preprocessing...\")\n",
        "df = df[(df['ap_hi'] > 0) & (df['ap_lo'] > 0)]\n",
        "df = df[(df['ap_hi'] < 250) & (df['ap_lo'] < 200)]\n",
        "df['age'] = (df['age'] / 365).astype(int)  # convert days to years\n",
        "\n",
        "X = df.drop(\"cardio\", axis=1)\n",
        "y = df[\"cardio\"]\n",
        "\n",
        "# 🧪 Apply SMOTE to balance classes\n",
        "print(\"🧪 Applying SMOTE to balance classes...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# 📊 Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
        ")\n",
        "\n",
        "# 🔄 Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# ⚙️ Define base models\n",
        "print(\"⚙️ Defining base models...\")\n",
        "base_models = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=150, max_depth=12, random_state=42)),\n",
        "    ('xgb', XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')),\n",
        "    ('lgb', LGBMClassifier(n_estimators=120, max_depth=6, learning_rate=0.1))\n",
        "]\n",
        "\n",
        "# 🧠 Define meta learner\n",
        "meta_model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300, random_state=42)\n",
        "\n",
        "# 🧠 Create stacking ensemble\n",
        "print(\"🧠 Combining all models with StackingClassifier...\")\n",
        "stack = StackingClassifier(estimators=base_models, final_estimator=meta_model, passthrough=True)\n",
        "\n",
        "# 🏋️ Train ensemble with tqdm\n",
        "print(\"🏋️ Training Ensemble...\")\n",
        "for _ in tqdm(range(1), desc=\"Training\"):\n",
        "    stack.fit(X_train, y_train)\n",
        "\n",
        "# 📈 Predict and evaluate\n",
        "y_pred = stack.predict(X_test)\n",
        "y_proba = stack.predict_proba(X_test)[:, 1]\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "roc = roc_auc_score(y_test, y_proba)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"\\n✅ Ensemble Accuracy: {acc:.4f}\")\n",
        "print(f\"✅ ROC-AUC Score: {roc:.4f}\")\n",
        "print(f\"🧾 Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "# 🔍 Sample prediction\n",
        "print(\"\\n🔎 Sample Prediction:\")\n",
        "sample = X_test[0].reshape(1, -1)\n",
        "sample_pred = stack.predict(sample)[0]\n",
        "risk = \"HIGH\" if sample_pred else \"LOW\"\n",
        "print(f\"🔍 Risk level: {risk} risk\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fc9e04520fe748d69722433386b02a72",
            "162ad7ee0ead434d8fdaecf9fc2313bb",
            "c0a74134c6794b33b19452dc19b907f2",
            "72d0101c749a487ab04791377d337e75",
            "6d90f2b1ff4f40389978b674a601583d",
            "e4c0e3bf7ff544da9cf52c2a0fe2aecc",
            "8256006adabd4ea08054202d7dfca7ae",
            "f562eabde33545e7b072800862701679",
            "97d513570cfe409c9c6354926e56cce0",
            "8b3c9946de5c4d5f869e50214d13c9e9",
            "c60113fd759943e5b4663e2f49ea107b"
          ]
        },
        "id": "YRoTSGxOpNr4",
        "outputId": "f7cca4aa-f0cb-4f5e-fedf-fe485025546d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading dataset from KaggleHub...\n",
            "📁 Path to dataset files: /kaggle/input/cardiovascular-disease-dataset\n",
            "📄 Loading dataset...\n",
            "🧼 Cleaning and preprocessing...\n",
            "🧪 Applying SMOTE to balance classes...\n",
            "⚙️ Defining base models...\n",
            "🧠 Combining all models with StackingClassifier...\n",
            "🏋️ Training Ensemble...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc9e04520fe748d69722433386b02a72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 27875, number of negative: 27875\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005698 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 545\n",
            "[LightGBM] [Info] Number of data points in the train set: 55750, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 22300, number of negative: 22300\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004287 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 526\n",
            "[LightGBM] [Info] Number of data points in the train set: 44600, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 22300, number of negative: 22300\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004209 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 532\n",
            "[LightGBM] [Info] Number of data points in the train set: 44600, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 22300, number of negative: 22300\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004253 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 537\n",
            "[LightGBM] [Info] Number of data points in the train set: 44600, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 22300, number of negative: 22300\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004205 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 531\n",
            "[LightGBM] [Info] Number of data points in the train set: 44600, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 22300, number of negative: 22300\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005027 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 534\n",
            "[LightGBM] [Info] Number of data points in the train set: 44600, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "✅ Ensemble Accuracy: 0.7004\n",
            "✅ ROC-AUC Score: 0.7511\n",
            "🧾 Confusion Matrix:\n",
            "[[5230 1739]\n",
            " [2437 4532]]\n",
            "\n",
            "🔎 Sample Prediction:\n",
            "🔍 Risk level: LOW risk\n"
          ]
        }
      ]
    }
  ]
}